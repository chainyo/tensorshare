{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd1d Getting Started","text":"TensorShare <p>\ud83e\udd1d Trade any tensors over the network</p> <p>TensorShare is a powerful tool enabling \u26a1 lightning-fast tensor sharing across networks.</p> <p>This project leverages the best open-source tools to provide a simple and easy-to-use interface for sharing tensors:</p> <ul> <li>safetensors for secure tensor serialization and deserialization.</li> <li>pydantic for data validation and settings management.</li> <li>fastapi for building APIs (and because it's too good to avoid it).</li> </ul> <p>This project is heavily in development and is not ready for production use. Feel free to contribute to the project by opening issues and pull requests.</p>"},{"location":"#usage","title":"Usage","text":"<p>Example of tensors serialization with torch:</p> <pre><code>import torch\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": torch.zeros((2, 2)),\n\"labels\": torch.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"torch\")\nprint(ts)\n# tensors=b'gAAAAAAAAAB7ImVtYmVkZGluZ3MiOnsiZHR5cGUiO...' size=168\n</code></pre> <p>You can now freely send the tensors over the network via any means (e.g. HTTP, gRPC, ...).</p> <p>On the other side, when you receive the tensors, you can deserialize them in any supported backend:</p> <pre><code>from tensorshare import Backend\nnp_tensors = ts.to_tensors(backend=Backend.NUMPY)\nprint(np_tensors)\n# {\n#   'embeddings': array([[0., 0.], [0., 0.]], dtype=float32),\n#   'labels': array([[0., 0.], [0., 0.]], dtype=float32)\n# }\n</code></pre> <p>For more examples and details, please refer to the Usage section.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Pydantic schema for sharing tensors ~&gt; TensorShare</li> <li> Serialization and deserialization ~&gt; tensorshare.serialization</li> <li> TensorProcessor for processing tensors ~&gt; TensorProcessor</li> <li> Server for sharing tensors ~&gt; TensorShareServer</li> <li> Client for sharing tensors ~&gt; TensorShareClient</li> <li> New incredible features! (Check the issues)</li> </ul>"},{"location":"installation/","title":"Installation","text":"<ul> <li>Python <code>3.8</code>, <code>3.9</code>, <code>3.10</code>, or <code>3.11</code> is required.</li> </ul>"},{"location":"installation/#pip","title":"Pip","text":"<pre><code>pip install tensorshare\n</code></pre>"},{"location":"installation/#poetry","title":"Poetry","text":"<pre><code>poetry add tensorshare\n</code></pre>"},{"location":"installation/#pipx","title":"Pipx","text":"<pre><code>pipx install tensorshare\n</code></pre>"},{"location":"installation/#tensorshare-modules","title":"TensorShare modules","text":"<p>TensorShare is a modular library. It means that you can install only the modules you need to reduce the installation time and the number of dependencies.</p> <p>Note</p> <p>We do care about your CI/CD pipelines. That's why we provide a way to install only the necessary modules. Only <code>safetensors</code> and <code>pydantic</code> are mandatory.</p>"},{"location":"installation/#client-module","title":"Client module","text":"<p>The client module is used to create a <code>TensorShareClient</code> for sending tensors to a FastAPI server.</p> <pre><code>pip install tensorshare[client]\n</code></pre> <p>Note</p> <p>It installs <code>aiohttp</code> on top of the main dependencies.</p>"},{"location":"installation/#server-module","title":"Server module","text":"<p>The server module is used to integrate TensorShare with a FastAPI server. </p> <pre><code>pip install tensorshare[server]\n</code></pre> <p>Note</p> <p>It installs <code>fastapi</code> on top of the main dependencies.</p> <p>Check the TensorShareServer and the FastAPI integration sections for more details.</p>"},{"location":"installation/#backend-installation","title":"Backend Installation","text":"<p>TensorShare is a framework-agnostic library. It means that the default installation does not include any framework and assumes that you will handle the backend (or backends) yourself.</p> <p>Note</p> <p>All the backends are optional and can be installed separately, but they all require <code>numpy</code>. That's why they all come with <code>numpy</code> as a dependency.</p> <p>However, we provide a set of backends that can be installed alongside TensorShare.</p> <ul> <li> Jax | <code>flax&gt;=0.6.3</code>, <code>jax&gt;=0.3.25</code>, <code>jaxlib&gt;=0.3.25</code></li> </ul> <pre><code>pip install tensorshare[jax]\n</code></pre> <ul> <li> NumPy | <code>numpy&gt;=1.21.6</code></li> </ul> <pre><code>pip install tensorshare[numpy]\n</code></pre> <ul> <li> PaddlePaddle | <code>paddlepaddle&gt;=2.4.1</code></li> </ul> <pre><code>pip install tensorshare[paddlepaddle]\n</code></pre> <ul> <li> Tensorflow | <code>tensorflow&gt;=2.14</code></li> </ul> <pre><code>pip install tensorshare[tensorflow]\n</code></pre> <ul> <li> PyTorch | <code>torch&gt;=1.10</code></li> </ul> <pre><code>pip install tensorshare[torch]\n</code></pre> <p>You can also install all the backends at once:</p> <pre><code>pip install tensorshare[all]\n</code></pre>"},{"location":"installation/#contributing","title":"Contributing","text":"<p>We use Hatch as the package manager for development.</p> <p>For installing the default environment:</p> <pre><code>hatch env create </code></pre> <p>The <code>quality</code> and <code>tests</code> environments are also available.  They will be auto-activated when running the corresponding commands.</p> <pre><code># Linting and formatting using black and ruff\nhatch run quality:format\n\n# Typechecking using mypy\nhatch run quality:typecheck\n\n# Testing using pytest\nhatch run tests:run\n</code></pre> <p>But you can also create them manually:</p> <pre><code>hatch env create quality\nhatch env create tests\n</code></pre>"},{"location":"license/","title":"\ud83d\udcdd License","text":"<p>MIT License</p> <p>Copyright (c) 2023-present Thomas Chaigneau t.chaigneau.tc@gmail.com</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/constants/","title":"tensorshare.Backend","text":"<p>Constants for tensorshare.serialization.</p>"},{"location":"api/constants/#src.tensorshare.serialization.constants.Backend","title":"<code>Backend</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Supported backends.</p> Source code in <code>src/tensorshare/serialization/constants.py</code> <pre><code>class Backend(str, Enum):\n\"\"\"Supported backends.\"\"\"\nFLAX = \"flax\"\nNUMPY = \"numpy\"\nPADDLEPADDLE = \"paddlepaddle\"\n# TENSORFLOW = \"tensorflow\"\nTORCH = \"torch\"\n</code></pre>"},{"location":"api/constants/#src.tensorshare.serialization.constants.TensorType","title":"<code>TensorType</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tensor types.</p> Source code in <code>src/tensorshare/serialization/constants.py</code> <pre><code>class TensorType(str, Enum):\n\"\"\"Tensor types.\"\"\"\nFLAX = \"jaxlib.xla_extension.ArrayImpl\"\nNUMPY = \"numpy.ndarray\"\nPADDLEPADDLE = \"paddle.Tensor\"\n# TENSORFLOW = \"tensorflow.Tensor\"\nTORCH = \"torch.Tensor\"\n</code></pre>"},{"location":"api/prepare_tensors/","title":"Utils functions","text":"<p>These functions are used to prepare tensors to be used in the <code>TensorShare</code> class.</p> <p>Prepare the tensors from any framework to a dictionary.</p> <p>This function is used to prepare the tensors from any framework to a dictionary. The dictionary will be used to create a TensorShare object. Use this function as a lazy way to prepare the tensors. If the data is a dictionary, the keys will be used as the tensor names. If the data is a list, a set, a tuple or any other iterable, the tensors will be named \"embeddings_{i}\" where i is the index of the tensor in the iterable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The tensors to prepare. It can be a dictionary, a generator, a list, a set, a tuple or any other single item.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The prepared data.</p> Source code in <code>src/tensorshare/utils.py</code> <pre><code>def prepare_tensors_to_dict(data: Any) -&gt; Dict[str, Any]:\n\"\"\"Prepare the tensors from any framework to a dictionary.\n    This function is used to prepare the tensors from any framework to a\n    dictionary. The dictionary will be used to create a TensorShare object.\n    Use this function as a lazy way to prepare the tensors.\n    If the data is a dictionary, the keys will be used as the tensor names.\n    If the data is a list, a set, a tuple or any other iterable, the tensors\n    will be named \"embeddings_{i}\" where i is the index of the tensor in the\n    iterable.\n    Args:\n        data (Any):\n            The tensors to prepare. It can be a dictionary, a generator,\n            a list, a set, a tuple or any other single item.\n    Returns:\n        Dict[str, Any]: The prepared data.\n    \"\"\"\nif isinstance(data, dict):\nreturn {str(key): tensor for key, tensor in data.items()}\nelif isinstance(data, (types.GeneratorType, list, set, tuple)) or hasattr(\ndata, \"__iter__\"\n):\n# The `hasattr(data, \"__iter__\")` check ensures that even if the input is some other kind of iterable\n# (not explicitly listed), it will still be processed here.\nreturn {f\"embeddings_{i}\": tensor for i, tensor in enumerate(data)}\nelse:\nreturn {\"embeddings\": data}\n</code></pre>"},{"location":"api/processor/","title":"TensorProcessor","text":"<p>Processor interface for converting between different tensor formats.</p>"},{"location":"api/processor/#src.tensorshare.serialization.processor.TensorProcessor","title":"<code>TensorProcessor</code>","text":"<p>Tensor processor class.</p> Source code in <code>src/tensorshare/serialization/processor.py</code> <pre><code>class TensorProcessor:\n\"\"\"Tensor processor class.\"\"\"\n@staticmethod\ndef serialize(\ntensors: Dict[\nstr,\nUnion[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"],\n],\nmetadata: Optional[Dict[str, str]] = None,\nbackend: Optional[Union[str, Backend]] = None,\n) -&gt; Tuple[bytes, ByteSize]:\n\"\"\"Serialize a dictionary of tensors to a tuple containing the serialized tensors and their size.\n        This method will convert a dictionary of tensors to a tuple containing the base64 encoded serialized\n        tensors and the size of the serialized tensors. It will use the backend if provided, otherwise it will\n        try to infer the backend from the tensors format.\n        Args:\n            tensors (Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]):\n                Tensors stored in a dictionary with their name as key.\n            metadata (Optional[Dict[str, str]], optional):\n                Metadata to add to the safetensors file. Defaults to None.\n            backend (Optional[Union[str, Backend]], optional):\n                Backend to use for the conversion. Defaults to None.\n                If None, the backend will be inferred from the tensors format.\n                Backend can be one of the following:\n                    - Backend.FLAX or 'flax'\n                    - Backend.NUMPY or 'numpy'\n                    - Backend.PADDLEPADDLE or 'paddlepaddle'\n                    - Backend.TENSORFLOW or 'tensorflow'\n                    - Backend.TORCH or 'torch'\n        Raises:\n            TypeError: If tensors is not a dictionary.\n            TypeError: If backend is not a string or an instance of Backend enum.\n            ValueError: If tensors is empty.\n            KeyError: If backend is not one of the supported backends.\n        Returns:\n            Tuple[bytes, ByteSize]:\n                A tuple containing the base64 encoded serialized tensors and the size of the serialized tensors.\n        \"\"\"\nif not isinstance(tensors, dict):\nlogger.warning(\nf\"Tensors should be a dictionary, got `{type(tensors)}` instead.\"\n\" Consider using the `prepare_tensors_to_dict` to lazy format \"\n\"your tensors. Check\"\n\" https://chainyo.github.io/tensorshare/usage/tensorshare/#lazy-tensors-formatting\"\n)\nraise TypeError\nelif not tensors:\nraise ValueError(\"Tensors dictionary cannot be empty.\")\nif backend is not None:\nif isinstance(backend, str):\ntry:\n_backend = Backend[backend.upper()]\nexcept KeyError as e:\nraise KeyError(\nf\"Invalid backend `{backend}`. Must be one of {list(Backend)}.\"\n) from e\nelif not isinstance(backend, Backend):\nraise TypeError(\n\"Backend must be a string or an instance of Backend enum, got\"\nf\" `{type(backend)}` instead. Use\"\n\" `tensorshare.serialization.Backend` to access the Backend enum.\"\n\" If you don't specify a backend, it will be inferred from the\"\n\" tensors format.\"\n)\nelse:\nlogger.warning(\n\"No backend specified. The backend will be inferred from the tensors\"\n\" format.\"\n\" If you want to specify the backend, use the `backend` argument. Check\"\n\" https://chainyo.github.io/tensorshare/usage/tensorshare/#with-a-specific-backend\"\n)\n_backend = _infer_backend(tensors)\n_tensors = _get_backend_method(_backend, \"serialize\")(\ntensors, metadata=metadata\n)\nreturn base64.b64encode(_tensors), ByteSize(len(_tensors))\n@staticmethod\ndef deserialize(\ndata: bytes,\nbackend: Union[str, Backend],\n) -&gt; Dict[\nstr, Union[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"]\n]:\n\"\"\"Deserialize base64 encoded serialized tensors to a dictionary of tensors.\n        This method will convert TensorShare.tensors to a dictionary of tensors with their name as key.\n        The backend must be specified in order to deserialize the data.\n        Args:\n            data (bytes):\n                The base64 encoded serialized tensors to deserialize.\n            backend (Union[str, Backend]):\n                The backend to use for the conversion. Must be one of the following:\n                    - Backend.FLAX or 'flax'\n                    - Backend.NUMPY or 'numpy'\n                    - Backend.PADDLEPADDLE or 'paddlepaddle'\n                    - Backend.TENSORFLOW or 'tensorflow'\n                    - Backend.TORCH or 'torch'\n        Raises:\n            TypeError: If data is not bytes.\n            TypeError: If backend is not a string or an instance of Backend enum.\n            KeyError: If backend is not one of the supported backends.\n        Returns:\n            Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]:\n                A dictionary of tensors in the specified backend with their name as key.\n        \"\"\"\nif not isinstance(data, bytes):\nraise TypeError(f\"Data must be bytes, got `{type(data)}` instead.\")\nif isinstance(backend, str):\ntry:\n_backend = Backend[backend.upper()]\nexcept KeyError as e:\nraise KeyError(\nf\"Invalid backend `{backend}`. Must be one of {list(Backend)}.\"\n) from e\nelif not isinstance(backend, Backend):\nraise TypeError(\n\"Backend must be a string or an instance of Backend enum, got\"\nf\" `{type(backend)}` instead. Use `tensorshare.serialization.Backend`\"\n\" to access the Backend enum.\"\n)\ntensors = _get_backend_method(_backend, \"deserialize\")(base64.b64decode(data))\nreturn tensors  # type: ignore\n</code></pre>"},{"location":"api/processor/#src.tensorshare.serialization.processor.TensorProcessor.deserialize","title":"<code>deserialize(data, backend)</code>  <code>staticmethod</code>","text":"<p>Deserialize base64 encoded serialized tensors to a dictionary of tensors.</p> <p>This method will convert TensorShare.tensors to a dictionary of tensors with their name as key. The backend must be specified in order to deserialize the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The base64 encoded serialized tensors to deserialize.</p> required <code>backend</code> <code>Union[str, Backend]</code> <p>The backend to use for the conversion. Must be one of the following:     - Backend.FLAX or 'flax'     - Backend.NUMPY or 'numpy'     - Backend.PADDLEPADDLE or 'paddlepaddle'     - Backend.TENSORFLOW or 'tensorflow'     - Backend.TORCH or 'torch'</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not bytes.</p> <code>TypeError</code> <p>If backend is not a string or an instance of Backend enum.</p> <code>KeyError</code> <p>If backend is not one of the supported backends.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[Array, ndarray, Tensor, Tensor, Tensor]]</code> <p>Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]: A dictionary of tensors in the specified backend with their name as key.</p> Source code in <code>src/tensorshare/serialization/processor.py</code> <pre><code>@staticmethod\ndef deserialize(\ndata: bytes,\nbackend: Union[str, Backend],\n) -&gt; Dict[\nstr, Union[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"]\n]:\n\"\"\"Deserialize base64 encoded serialized tensors to a dictionary of tensors.\n    This method will convert TensorShare.tensors to a dictionary of tensors with their name as key.\n    The backend must be specified in order to deserialize the data.\n    Args:\n        data (bytes):\n            The base64 encoded serialized tensors to deserialize.\n        backend (Union[str, Backend]):\n            The backend to use for the conversion. Must be one of the following:\n                - Backend.FLAX or 'flax'\n                - Backend.NUMPY or 'numpy'\n                - Backend.PADDLEPADDLE or 'paddlepaddle'\n                - Backend.TENSORFLOW or 'tensorflow'\n                - Backend.TORCH or 'torch'\n    Raises:\n        TypeError: If data is not bytes.\n        TypeError: If backend is not a string or an instance of Backend enum.\n        KeyError: If backend is not one of the supported backends.\n    Returns:\n        Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]:\n            A dictionary of tensors in the specified backend with their name as key.\n    \"\"\"\nif not isinstance(data, bytes):\nraise TypeError(f\"Data must be bytes, got `{type(data)}` instead.\")\nif isinstance(backend, str):\ntry:\n_backend = Backend[backend.upper()]\nexcept KeyError as e:\nraise KeyError(\nf\"Invalid backend `{backend}`. Must be one of {list(Backend)}.\"\n) from e\nelif not isinstance(backend, Backend):\nraise TypeError(\n\"Backend must be a string or an instance of Backend enum, got\"\nf\" `{type(backend)}` instead. Use `tensorshare.serialization.Backend`\"\n\" to access the Backend enum.\"\n)\ntensors = _get_backend_method(_backend, \"deserialize\")(base64.b64decode(data))\nreturn tensors  # type: ignore\n</code></pre>"},{"location":"api/processor/#src.tensorshare.serialization.processor.TensorProcessor.serialize","title":"<code>serialize(tensors, metadata=None, backend=None)</code>  <code>staticmethod</code>","text":"<p>Serialize a dictionary of tensors to a tuple containing the serialized tensors and their size.</p> <p>This method will convert a dictionary of tensors to a tuple containing the base64 encoded serialized tensors and the size of the serialized tensors. It will use the backend if provided, otherwise it will try to infer the backend from the tensors format.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, Union[Array, ndarray, Tensor, Tensor, Tensor]]</code> <p>Tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[Union[str, Backend]]</code> <p>Backend to use for the conversion. Defaults to None. If None, the backend will be inferred from the tensors format. Backend can be one of the following:     - Backend.FLAX or 'flax'     - Backend.NUMPY or 'numpy'     - Backend.PADDLEPADDLE or 'paddlepaddle'     - Backend.TENSORFLOW or 'tensorflow'     - Backend.TORCH or 'torch'</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If tensors is not a dictionary.</p> <code>TypeError</code> <p>If backend is not a string or an instance of Backend enum.</p> <code>ValueError</code> <p>If tensors is empty.</p> <code>KeyError</code> <p>If backend is not one of the supported backends.</p> <p>Returns:</p> Type Description <code>Tuple[bytes, ByteSize]</code> <p>Tuple[bytes, ByteSize]: A tuple containing the base64 encoded serialized tensors and the size of the serialized tensors.</p> Source code in <code>src/tensorshare/serialization/processor.py</code> <pre><code>@staticmethod\ndef serialize(\ntensors: Dict[\nstr,\nUnion[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"],\n],\nmetadata: Optional[Dict[str, str]] = None,\nbackend: Optional[Union[str, Backend]] = None,\n) -&gt; Tuple[bytes, ByteSize]:\n\"\"\"Serialize a dictionary of tensors to a tuple containing the serialized tensors and their size.\n    This method will convert a dictionary of tensors to a tuple containing the base64 encoded serialized\n    tensors and the size of the serialized tensors. It will use the backend if provided, otherwise it will\n    try to infer the backend from the tensors format.\n    Args:\n        tensors (Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]):\n            Tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n        backend (Optional[Union[str, Backend]], optional):\n            Backend to use for the conversion. Defaults to None.\n            If None, the backend will be inferred from the tensors format.\n            Backend can be one of the following:\n                - Backend.FLAX or 'flax'\n                - Backend.NUMPY or 'numpy'\n                - Backend.PADDLEPADDLE or 'paddlepaddle'\n                - Backend.TENSORFLOW or 'tensorflow'\n                - Backend.TORCH or 'torch'\n    Raises:\n        TypeError: If tensors is not a dictionary.\n        TypeError: If backend is not a string or an instance of Backend enum.\n        ValueError: If tensors is empty.\n        KeyError: If backend is not one of the supported backends.\n    Returns:\n        Tuple[bytes, ByteSize]:\n            A tuple containing the base64 encoded serialized tensors and the size of the serialized tensors.\n    \"\"\"\nif not isinstance(tensors, dict):\nlogger.warning(\nf\"Tensors should be a dictionary, got `{type(tensors)}` instead.\"\n\" Consider using the `prepare_tensors_to_dict` to lazy format \"\n\"your tensors. Check\"\n\" https://chainyo.github.io/tensorshare/usage/tensorshare/#lazy-tensors-formatting\"\n)\nraise TypeError\nelif not tensors:\nraise ValueError(\"Tensors dictionary cannot be empty.\")\nif backend is not None:\nif isinstance(backend, str):\ntry:\n_backend = Backend[backend.upper()]\nexcept KeyError as e:\nraise KeyError(\nf\"Invalid backend `{backend}`. Must be one of {list(Backend)}.\"\n) from e\nelif not isinstance(backend, Backend):\nraise TypeError(\n\"Backend must be a string or an instance of Backend enum, got\"\nf\" `{type(backend)}` instead. Use\"\n\" `tensorshare.serialization.Backend` to access the Backend enum.\"\n\" If you don't specify a backend, it will be inferred from the\"\n\" tensors format.\"\n)\nelse:\nlogger.warning(\n\"No backend specified. The backend will be inferred from the tensors\"\n\" format.\"\n\" If you want to specify the backend, use the `backend` argument. Check\"\n\" https://chainyo.github.io/tensorshare/usage/tensorshare/#with-a-specific-backend\"\n)\n_backend = _infer_backend(tensors)\n_tensors = _get_backend_method(_backend, \"serialize\")(\ntensors, metadata=metadata\n)\nreturn base64.b64encode(_tensors), ByteSize(len(_tensors))\n</code></pre>"},{"location":"api/serialization/","title":"Serialization","text":"<p>All the serialization and deserialization methods used by the TensorProcessor.</p>"},{"location":"api/serialization/#flax","title":"Flax","text":"<p>Flax serialization and deserialization utilities.</p>"},{"location":"api/serialization/#src.tensorshare.serialization.flax.deserialize","title":"<code>deserialize(data)</code>","text":"<p>Convert safetensors format to flax tensors using <code>safetensors.flax.load</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Safetensors formatted data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Array]</code> <p>Dict[str, Array]: Flax tensors stored in a dictionary with their name as key.</p> Source code in <code>src/tensorshare/serialization/flax.py</code> <pre><code>@require_backend(\"flax\", \"flaxlib\", \"jax\")\ndef deserialize(data: bytes) -&gt; Dict[str, Array]:\n\"\"\"\n    Convert safetensors format to flax tensors using `safetensors.flax.load`.\n    Args:\n        data (bytes): Safetensors formatted data.\n    Returns:\n        Dict[str, Array]: Flax tensors stored in a dictionary with their name as key.\n    \"\"\"\nreturn flax_load(data)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#src.tensorshare.serialization.flax.serialize","title":"<code>serialize(tensors, metadata=None)</code>","text":"<p>Convert flax tensors to safetensors format using <code>safetensors.flax.save</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, Array]</code> <p>Flax tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Tensors formatted with their metadata if any.</p> Source code in <code>src/tensorshare/serialization/flax.py</code> <pre><code>@require_backend(\"flax\", \"flaxlib\", \"jax\")\ndef serialize(\ntensors: Dict[str, Array], metadata: Optional[Dict[str, str]] = None\n) -&gt; bytes:\n\"\"\"\n    Convert flax tensors to safetensors format using `safetensors.flax.save`.\n    Args:\n        tensors (Dict[str, Array]):\n            Flax tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n    Returns:\n        bytes: Tensors formatted with their metadata if any.\n    \"\"\"\nreturn flax_save(tensors, metadata=metadata)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#numpy","title":"NumPy","text":"<p>NumPy serialization and deserialization utilities.</p>"},{"location":"api/serialization/#src.tensorshare.serialization.numpy.deserialize","title":"<code>deserialize(data)</code>","text":"<p>Convert safetensors format to numpy tensors using <code>safetensors.numpy.load</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Safetensors formatted data.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Numpy tensors stored in a dictionary with their name as key.</p> Source code in <code>src/tensorshare/serialization/numpy.py</code> <pre><code>@require_backend(\"numpy\")\ndef deserialize(data: bytes) -&gt; Dict[str, np.ndarray]:\n\"\"\"\n    Convert safetensors format to numpy tensors using `safetensors.numpy.load`.\n    Args:\n        data (bytes): Safetensors formatted data.\n    Returns:\n        Dict[str, np.ndarray]: Numpy tensors stored in a dictionary with their name as key.\n    \"\"\"\nreturn np_load(data)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#src.tensorshare.serialization.numpy.serialize","title":"<code>serialize(tensors, metadata=None)</code>","text":"<p>Convert numpy tensors to safetensors format using <code>safetensors.numpy.save</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, ndarray]</code> <p>Numpy tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Tensors formatted with their metadata if any.</p> Source code in <code>src/tensorshare/serialization/numpy.py</code> <pre><code>@require_backend(\"numpy\")\ndef serialize(\ntensors: Dict[str, np.ndarray], metadata: Optional[Dict[str, str]] = None\n) -&gt; bytes:\n\"\"\"\n    Convert numpy tensors to safetensors format using `safetensors.numpy.save`.\n    Args:\n        tensors (Dict[str, np.ndarray]):\n            Numpy tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n    Returns:\n        bytes: Tensors formatted with their metadata if any.\n    \"\"\"\nreturn np_save(tensors, metadata=metadata)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#paddlepaddle","title":"PaddlePaddle","text":"<p>PaddlePaddle serialization and deserialization utilities</p>"},{"location":"api/serialization/#src.tensorshare.serialization.paddle.deserialize","title":"<code>deserialize(data)</code>","text":"<p>Convert safetensors format to paddle tensors using <code>safetensors.paddle.load</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Safetensors formatted data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, paddle.Tensor]: Paddle tensors stored in a dictionary with their name as key.</p> Source code in <code>src/tensorshare/serialization/paddle.py</code> <pre><code>@require_backend(\"paddle\")\ndef deserialize(data: bytes) -&gt; Dict[str, paddle.Tensor]:\n\"\"\"\n    Convert safetensors format to paddle tensors using `safetensors.paddle.load`.\n    Args:\n        data (bytes): Safetensors formatted data.\n    Returns:\n        Dict[str, paddle.Tensor]: Paddle tensors stored in a dictionary with their name as key.\n    \"\"\"\nreturn paddle_load(data)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#src.tensorshare.serialization.paddle.serialize","title":"<code>serialize(tensors, metadata=None)</code>","text":"<p>Convert paddle tensors to safetensors format using <code>safetensors.paddle.save</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, Tensor]</code> <p>Paddle tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Tensors formatted with their metadata if any.</p> Source code in <code>src/tensorshare/serialization/paddle.py</code> <pre><code>@require_backend(\"paddle\")\ndef serialize(\ntensors: Dict[str, paddle.Tensor], metadata: Optional[Dict[str, str]] = None\n) -&gt; bytes:\n\"\"\"\n    Convert paddle tensors to safetensors format using `safetensors.paddle.save`.\n    Args:\n        tensors (Dict[str, paddle.Tensor]):\n            Paddle tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n    Returns:\n        bytes: Tensors formatted with their metadata if any.\n    \"\"\"\nreturn paddle_save(tensors, metadata=metadata)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#pytorch","title":"PyTorch","text":"<p>Torch serialization and deserialization utilities.</p>"},{"location":"api/serialization/#src.tensorshare.serialization.torch.deserialize","title":"<code>deserialize(data)</code>","text":"<p>Convert safetensors format to torch tensors using <code>safetensors.torch.load</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Safetensors formatted data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor]: Torch tensors stored in a dictionary with their name as key.</p> Source code in <code>src/tensorshare/serialization/torch.py</code> <pre><code>@require_backend(\"torch\")\ndef deserialize(data: bytes) -&gt; Dict[str, torch.Tensor]:\n\"\"\"\n    Convert safetensors format to torch tensors using `safetensors.torch.load`.\n    Args:\n        data (bytes): Safetensors formatted data.\n    Returns:\n        Dict[str, torch.Tensor]: Torch tensors stored in a dictionary with their name as key.\n    \"\"\"\nreturn torch_load(data)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#src.tensorshare.serialization.torch.serialize","title":"<code>serialize(tensors, metadata=None)</code>","text":"<p>Convert torch tensors to safetensors format using <code>safetensors.torch.save</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, Tensor]</code> <p>Torch tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Tensors formatted with their metadata if any.</p> Source code in <code>src/tensorshare/serialization/torch.py</code> <pre><code>@require_backend(\"torch\")\ndef serialize(\ntensors: Dict[str, torch.Tensor], metadata: Optional[Dict[str, str]] = None\n) -&gt; bytes:\n\"\"\"\n    Convert torch tensors to safetensors format using `safetensors.torch.save`.\n    Args:\n        tensors (Dict[str, torch.Tensor]):\n            Torch tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n    Returns:\n        bytes: Tensors formatted with their metadata if any.\n    \"\"\"\nreturn torch_save(tensors, metadata=metadata)  # type: ignore\n</code></pre>"},{"location":"api/serialization/#tensorflow","title":"TensorFlow","text":"<p>Tensorflow serialization and deserialization utilities.</p>"},{"location":"api/tensorshare/","title":"TensorShare","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base model for tensor sharing.</p> Source code in <code>src/tensorshare/schema.py</code> <pre><code>class TensorShare(BaseModel):\n\"\"\"Base model for tensor sharing.\"\"\"\ntensors: bytes\nsize: ByteSize\nmodel_config = ConfigDict(\nextra=\"forbid\",\nfrozen=True,\njson_schema_extra={\n\"tensors\": b\"SAAAAAAAAAB7ImVtYmVkZGluZ3MiOnsiZHR5cGUiOiJGMzIiLCJzaGFwZSI6WzEsMV0sImRhdGFfb2Zmc2V0cyI6WzAsNF19fSAgICAgICAAAAAA\",\n\"size\": 84,\n},\n)\n@property\ndef size_as_str(self) -&gt; str:\n\"\"\"Return size as a string in human readable format.\"\"\"\nreturn self.size.human_readable()\n@classmethod\ndef from_dict(\ncls,\ntensors: Dict[\nstr,\nUnion[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"],\n],\nmetadata: Optional[Dict[str, str]] = None,\nbackend: Optional[Union[str, Backend]] = None,\n) -&gt; TensorShare:\n\"\"\"\n        Create a TensorShare object from a dictionary of tensors.\n        Args:\n            tensors (Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]):\n                Tensors stored in a dictionary with their name as key.\n            metadata (Optional[Dict[str, str]], optional):\n                Metadata to add to the safetensors file. Defaults to None.\n            backend (Optional[Union[str, Backend]], optional):\n                Backend to use for the conversion. Defaults to None.\n        Raises:\n            TypeError: If tensors is not a dictionary.\n            TypeError: If backend is not a string or an instance of Backend enum.\n            ValueError: If backend is not supported.\n            ValueError: If tensors is empty.\n        Returns:\n            TensorShare: A TensorShare object.\n        \"\"\"\n_tensors, size = TensorProcessor.serialize(\ntensors=tensors, metadata=metadata, backend=backend\n)\nreturn cls(tensors=_tensors, size=size)\ndef to_tensors(\nself,\nbackend: Union[str, Backend],\n) -&gt; Dict[\nstr, Union[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"]\n]:\n\"\"\"\n        Convert a TensorShare object to a dictionary of tensors in the specified backend.\n        Args:\n            backend (Union[str, Backend]): Backend to use for the conversion.\n        Raises:\n            TypeError: If backend is not a string or an instance of Backend enum.\n            ValueError: If backend is not supported.\n        Returns:\n            Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]:\n                Tensors stored in a dictionary with their name as key in the specified backend.\n        \"\"\"\nreturn TensorProcessor.deserialize(self.tensors, backend=backend)\n</code></pre>"},{"location":"api/tensorshare/#src.tensorshare.schema.TensorShare.size_as_str","title":"<code>size_as_str: str</code>  <code>property</code>","text":"<p>Return size as a string in human readable format.</p>"},{"location":"api/tensorshare/#src.tensorshare.schema.TensorShare.from_dict","title":"<code>from_dict(tensors, metadata=None, backend=None)</code>  <code>classmethod</code>","text":"<p>Create a TensorShare object from a dictionary of tensors.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Dict[str, Union[Array, ndarray, Tensor, Tensor, Tensor]]</code> <p>Tensors stored in a dictionary with their name as key.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>Metadata to add to the safetensors file. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[Union[str, Backend]]</code> <p>Backend to use for the conversion. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If tensors is not a dictionary.</p> <code>TypeError</code> <p>If backend is not a string or an instance of Backend enum.</p> <code>ValueError</code> <p>If backend is not supported.</p> <code>ValueError</code> <p>If tensors is empty.</p> <p>Returns:</p> Name Type Description <code>TensorShare</code> <code>TensorShare</code> <p>A TensorShare object.</p> Source code in <code>src/tensorshare/schema.py</code> <pre><code>@classmethod\ndef from_dict(\ncls,\ntensors: Dict[\nstr,\nUnion[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"],\n],\nmetadata: Optional[Dict[str, str]] = None,\nbackend: Optional[Union[str, Backend]] = None,\n) -&gt; TensorShare:\n\"\"\"\n    Create a TensorShare object from a dictionary of tensors.\n    Args:\n        tensors (Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]):\n            Tensors stored in a dictionary with their name as key.\n        metadata (Optional[Dict[str, str]], optional):\n            Metadata to add to the safetensors file. Defaults to None.\n        backend (Optional[Union[str, Backend]], optional):\n            Backend to use for the conversion. Defaults to None.\n    Raises:\n        TypeError: If tensors is not a dictionary.\n        TypeError: If backend is not a string or an instance of Backend enum.\n        ValueError: If backend is not supported.\n        ValueError: If tensors is empty.\n    Returns:\n        TensorShare: A TensorShare object.\n    \"\"\"\n_tensors, size = TensorProcessor.serialize(\ntensors=tensors, metadata=metadata, backend=backend\n)\nreturn cls(tensors=_tensors, size=size)\n</code></pre>"},{"location":"api/tensorshare/#src.tensorshare.schema.TensorShare.to_tensors","title":"<code>to_tensors(backend)</code>","text":"<p>Convert a TensorShare object to a dictionary of tensors in the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Union[str, Backend]</code> <p>Backend to use for the conversion.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If backend is not a string or an instance of Backend enum.</p> <code>ValueError</code> <p>If backend is not supported.</p> <p>Returns:</p> Type Description <code>Dict[str, Union['Array', 'np.ndarray', 'paddle.Tensor', 'tf.Tensor', 'torch.Tensor']]</code> <p>Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]: Tensors stored in a dictionary with their name as key in the specified backend.</p> Source code in <code>src/tensorshare/schema.py</code> <pre><code>def to_tensors(\nself,\nbackend: Union[str, Backend],\n) -&gt; Dict[\nstr, Union[\"Array\", \"np.ndarray\", \"paddle.Tensor\", \"tf.Tensor\", \"torch.Tensor\"]\n]:\n\"\"\"\n    Convert a TensorShare object to a dictionary of tensors in the specified backend.\n    Args:\n        backend (Union[str, Backend]): Backend to use for the conversion.\n    Raises:\n        TypeError: If backend is not a string or an instance of Backend enum.\n        ValueError: If backend is not supported.\n    Returns:\n        Dict[str, Union[Array, np.ndarray, paddle.Tensor, tf.Tensor, torch.Tensor]]:\n            Tensors stored in a dictionary with their name as key in the specified backend.\n    \"\"\"\nreturn TensorProcessor.deserialize(self.tensors, backend=backend)\n</code></pre>"},{"location":"api/tensorshare_client/","title":"tensorshare.TensorShareClient","text":"<p>Async http client for sending tensors to a remote server.</p>"},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient","title":"<code>TensorShareClient</code>","text":"<p>Asynchronous Client for sending tensors to a remote server.</p> <p>This client uses the aiohttp library which is an asynchronous http client, built on top of asyncio. But the client provides a synchronous interface for the users through the use of the <code>asyncio.run</code> function.</p> Note <p>Using the synchronous interface will block the main thread until the request is completed and the response is received. It's not recommended to use the synchronous interface in a production environment as it may cause performance issues. The synchronous interface is only provided for testing purposes.</p> Example <p>import torch from tensorshare import TensorShare, TensorShareClient, TensorShareServer</p> <p>server_config = TensorShareServer(\"http://localhost:8765\") client = TensorShareClient(server_config)</p>"},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--synchronous-interface","title":"Synchronous interface","text":"<p>print(client.ping_server())</p>"},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--true","title":"True","text":"<p>ts = TensorShare.from_dict({\"embeddings\": torch.rand(10, 10)}) print(client.send_tensor(ts))</p>"},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--_1","title":"","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--asynchronous-interface","title":"Asynchronous interface <p>import asyncio</p> <p>async def main(): ...     r = await client.async_ping_server() ...     print(r) ...     r = await client.async_send_tensor(ts) ...     print(r)</p> <p>asyncio.run(main())</p>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--true_1","title":"True","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient--_2","title":"Source code in <code>src/tensorshare/client.py</code> <pre><code>class TensorShareClient:\n    \"\"\"Asynchronous Client for sending tensors to a remote server.\n\n    This client uses the aiohttp library which is an asynchronous http client,\n    built on top of asyncio. But the client provides a synchronous interface\n    for the users through the use of the `asyncio.run` function.\n\n    Note:\n        Using the synchronous interface will block the main thread until the\n        request is completed and the response is received. It's not recommended\n        to use the synchronous interface in a production environment as it\n        may cause performance issues. The synchronous interface is only\n        provided for testing purposes.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from tensorshare import TensorShare, TensorShareClient, TensorShareServer\n\n        &gt;&gt;&gt; server_config = TensorShareServer(\"http://localhost:8765\")\n        &gt;&gt;&gt; client = TensorShareClient(server_config)\n\n        &gt;&gt;&gt; # Synchronous interface\n        &gt;&gt;&gt; print(client.ping_server())\n        &gt;&gt;&gt; # True\n\n        &gt;&gt;&gt; ts = TensorShare.from_dict({\"embeddings\": torch.rand(10, 10)})\n        &gt;&gt;&gt; print(client.send_tensor(ts))\n        &gt;&gt;&gt; # &lt;ClientResponse(http://localhost:8765/receive_tensor) [200 OK]&gt;\n\n        &gt;&gt;&gt; # Asynchronous interface\n        &gt;&gt;&gt; import asyncio\n\n        &gt;&gt;&gt; async def main():\n        ...     r = await client.async_ping_server()\n        ...     print(r)\n        ...     r = await client.async_send_tensor(ts)\n        ...     print(r)\n\n        &gt;&gt;&gt; asyncio.run(main())\n        &gt;&gt;&gt; # True\n        &gt;&gt;&gt; # &lt;ClientResponse(http://localhost:8765/receive_tensor) [200 OK]&gt;\n    \"\"\"\n\n    def __init__(\n        self,\n        server_config: TensorShareServer,\n        timeout: int = 10,\n        validate_endpoints: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the client.\n\n        Args:\n            server_config (TensorShareServer):\n                The server configuration to use for sending tensors.\n            timeout (int):\n                The timeout in seconds for the http requests. Defaults to 10.\n            validate_endpoints (bool):\n                Whether to validate the endpoints on the server at the time of\n                initialization. Defaults to True.\n        \"\"\"\n        self.server = server_config\n        self.timeout = timeout\n\n        if validate_endpoints:\n            self._validate_endpoints()\n\n    def ping_server(self) -&gt; bool:\n        \"\"\"Ping the server to check if it is available.\"\"\"\n        logger.warning_once(\n            \"Using the synchronous interface for the client is not recommended.\"\n            \" It may cause performance issues, and is only provided for testing.\"\n            \" Please use the asynchronous interface instead: `async_ping_server`.\"\n        )\n        return asyncio.run(self.async_ping_server())\n\n    async def async_ping_server(self) -&gt; bool:\n        \"\"\"Ping the server to check if it is available.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            response = await session.get(str(self.server.ping), timeout=self.timeout)\n\n        _is_available: bool = response.status == 200\n\n        return _is_available\n\n    def send_tensor(self, tensor_data: TensorShare) -&gt; aiohttp.ClientResponse:\n        \"\"\"Send a TensorShare object to the server using aiohttp.\n\n        Args:\n            tensor_data (TensorShare):\n                The tensor data to send to the server.\n\n        Returns:\n            aiohttp.ClientResponse: The response from the server.\n        \"\"\"\n        logger.warning_once(\n            \"Using the synchronous interface for the client is not recommended.\"\n            \" It may cause performance issues, and is only provided for testing.\"\n            \" Please use the asynchronous interface instead: `async_send_tensor`.\"\n        )\n        return asyncio.run(self.async_send_tensor(tensor_data))\n\n    async def async_send_tensor(\n        self, tensor_data: TensorShare\n    ) -&gt; aiohttp.ClientResponse:\n        \"\"\"\n        Send a TensorShare object to the server using aiohttp.\n\n        Args:\n            tensor_data (TensorShare):\n                The tensor data to send to the server.\n\n        Returns:\n            aiohttp.ClientResponse: The response from the server.\n        \"\"\"\n        if not isinstance(tensor_data, TensorShare):\n            raise TypeError(\n                \"Expected tensor_data to be of type TensorShare, got\"\n                f\" {type(tensor_data)}\"\n            )\n\n        async with aiohttp.ClientSession() as session:\n            response = await session.post(\n                str(self.server.receive_tensor),\n                headers={\"Content-Type\": \"application/json\"},\n                data=tensor_data.model_dump_json(),\n                timeout=self.timeout,\n            )\n\n        return response\n\n    def _validate_endpoints(self) -&gt; None:\n        \"\"\"\n        Check that all the registered endpoints are available on the server.\n\n        Raises:\n            AssertionError: If any of the endpoints are not available.\n        \"\"\"\n        logger.info(\"Validating endpoints on the server...\")\n\n        try:\n            assert asyncio.run(self.async_ping_server()) is True\n            logger.info(\"ping endpoit \u2705\")\n\n            assert self.send_tensor(fake_tensorshare_data()).status == 200\n            logger.info(\"receive_tensor endpoint \u2705\")\n\n        except Exception as e:\n            raise AssertionError(\n                f\"Could not connect to the server at {self.server.url}\\n{e}\"\n            ) from e\n</code></pre>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient.__init__","title":"<code>__init__(server_config, timeout=10, validate_endpoints=True)</code>   <p>Initialize the client.</p> <p>Parameters:</p>    Name Type Description Default     <code>server_config</code>  <code>TensorShareServer</code>    <p>The server configuration to use for sending tensors.</p>    required    <code>timeout</code>  <code>int</code>    <p>The timeout in seconds for the http requests. Defaults to 10.</p>    <code>10</code>    <code>validate_endpoints</code>  <code>bool</code>    <p>Whether to validate the endpoints on the server at the time of initialization. Defaults to True.</p>    <code>True</code>      Source code in <code>src/tensorshare/client.py</code> <pre><code>def __init__(\n    self,\n    server_config: TensorShareServer,\n    timeout: int = 10,\n    validate_endpoints: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the client.\n\n    Args:\n        server_config (TensorShareServer):\n            The server configuration to use for sending tensors.\n        timeout (int):\n            The timeout in seconds for the http requests. Defaults to 10.\n        validate_endpoints (bool):\n            Whether to validate the endpoints on the server at the time of\n            initialization. Defaults to True.\n    \"\"\"\n    self.server = server_config\n    self.timeout = timeout\n\n    if validate_endpoints:\n        self._validate_endpoints()\n</code></pre>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient.async_ping_server","title":"<code>async_ping_server()</code>  <code>async</code>    <p>Ping the server to check if it is available.</p>  Source code in <code>src/tensorshare/client.py</code> <pre><code>async def async_ping_server(self) -&gt; bool:\n    \"\"\"Ping the server to check if it is available.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        response = await session.get(str(self.server.ping), timeout=self.timeout)\n\n    _is_available: bool = response.status == 200\n\n    return _is_available\n</code></pre>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient.async_send_tensor","title":"<code>async_send_tensor(tensor_data)</code>  <code>async</code>    <p>Send a TensorShare object to the server using aiohttp.</p> <p>Parameters:</p>    Name Type Description Default     <code>tensor_data</code>  <code>TensorShare</code>    <p>The tensor data to send to the server.</p>    required     <p>Returns:</p>    Type Description      <code>ClientResponse</code>    <p>aiohttp.ClientResponse: The response from the server.</p>       Source code in <code>src/tensorshare/client.py</code> <pre><code>async def async_send_tensor(\n    self, tensor_data: TensorShare\n) -&gt; aiohttp.ClientResponse:\n    \"\"\"\n    Send a TensorShare object to the server using aiohttp.\n\n    Args:\n        tensor_data (TensorShare):\n            The tensor data to send to the server.\n\n    Returns:\n        aiohttp.ClientResponse: The response from the server.\n    \"\"\"\n    if not isinstance(tensor_data, TensorShare):\n        raise TypeError(\n            \"Expected tensor_data to be of type TensorShare, got\"\n            f\" {type(tensor_data)}\"\n        )\n\n    async with aiohttp.ClientSession() as session:\n        response = await session.post(\n            str(self.server.receive_tensor),\n            headers={\"Content-Type\": \"application/json\"},\n            data=tensor_data.model_dump_json(),\n            timeout=self.timeout,\n        )\n\n    return response\n</code></pre>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient.ping_server","title":"<code>ping_server()</code>   <p>Ping the server to check if it is available.</p>  Source code in <code>src/tensorshare/client.py</code> <pre><code>def ping_server(self) -&gt; bool:\n    \"\"\"Ping the server to check if it is available.\"\"\"\n    logger.warning_once(\n        \"Using the synchronous interface for the client is not recommended.\"\n        \" It may cause performance issues, and is only provided for testing.\"\n        \" Please use the asynchronous interface instead: `async_ping_server`.\"\n    )\n    return asyncio.run(self.async_ping_server())\n</code></pre>","text":""},{"location":"api/tensorshare_client/#src.tensorshare.client.TensorShareClient.send_tensor","title":"<code>send_tensor(tensor_data)</code>   <p>Send a TensorShare object to the server using aiohttp.</p> <p>Parameters:</p>    Name Type Description Default     <code>tensor_data</code>  <code>TensorShare</code>    <p>The tensor data to send to the server.</p>    required     <p>Returns:</p>    Type Description      <code>ClientResponse</code>    <p>aiohttp.ClientResponse: The response from the server.</p>       Source code in <code>src/tensorshare/client.py</code> <pre><code>def send_tensor(self, tensor_data: TensorShare) -&gt; aiohttp.ClientResponse:\n    \"\"\"Send a TensorShare object to the server using aiohttp.\n\n    Args:\n        tensor_data (TensorShare):\n            The tensor data to send to the server.\n\n    Returns:\n        aiohttp.ClientResponse: The response from the server.\n    \"\"\"\n    logger.warning_once(\n        \"Using the synchronous interface for the client is not recommended.\"\n        \" It may cause performance issues, and is only provided for testing.\"\n        \" Please use the asynchronous interface instead: `async_send_tensor`.\"\n    )\n    return asyncio.run(self.async_send_tensor(tensor_data))\n</code></pre>","text":""},{"location":"api/tensorshare_server/","title":"tensorshare.TensorShareServer","text":"<p>The Server module introduces all the routing, functions and dependencies for server side.</p>"},{"location":"api/tensorshare_server/#src.tensorshare.server.create_async_tensorshare_router","title":"<code>create_async_tensorshare_router(server_config=None, custom_operation=None)</code>","text":"<p>Create and configure a new async TensorShare router using the given configuration and computation operation.</p> <p>Parameters:</p> Name Type Description Default <code>server_config</code> <code>Optional[Dict[str, Union[str, Type[BaseModel]]]]</code> <p>The server configuration to use for the router.</p> <code>None</code> <code>custom_operation</code> <code>Optional[Callable[[TensorShare], Awaitable[Any]]]</code> <p>The async custom computation operation to use for the <code>/receive_tensor</code> endpoint.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>APIRouter</code> <code>APIRouter</code> <p>The configured TensorShare router ready to be integrated in a FastAPI app.</p> Source code in <code>src/tensorshare/server.py</code> <pre><code>def create_async_tensorshare_router(\nserver_config: Optional[Dict[str, Union[str, Type[BaseModel]]]] = None,\ncustom_operation: Optional[Callable[[TensorShare], Awaitable[Any]]] = None,\n) -&gt; APIRouter:\n\"\"\"\n    Create and configure a new async TensorShare router using the given configuration and computation operation.\n    Args:\n        server_config (Optional[Dict[str, Union[str, Type[BaseModel]]]]):\n            The server configuration to use for the router.\n        custom_operation (Optional[Callable[[TensorShare], Awaitable[Any]]]):\n            The async custom computation operation to use for the `/receive_tensor` endpoint.\n    Returns:\n        APIRouter:\n            The configured TensorShare router ready to be integrated in a FastAPI app.\n    \"\"\"\nif custom_operation is not None and not inspect.iscoroutinefunction(\ncustom_operation\n):\nraise TypeError(\n\"The `custom_operation` callable must be asynchronous. \"\n\"Please use the `create_tensorshare_router` function instead.\"\n)\nrouter = APIRouter()\n_default_config: Dict[str, Union[str, Type[BaseModel]]] = {\n\"url\": \"http://localhost:8000\",\n\"response_model\": DefaultResponse,\n}\nconfig: TensorShareServer = TensorShareServer.from_dict(\nserver_config=server_config or _default_config\n)\ntarget_operation: Callable[[TensorShare], Awaitable[Any]] = (\ncustom_operation or default_async_compute_operation\n)\ndef get_computation_operation() -&gt; Callable[[TensorShare], Awaitable[Any]]:\n\"\"\"Dependency that returns the currently set async computation function.\"\"\"\nreturn target_operation\n@router.post(\nf\"{config.receive_tensor.path}\",\nresponse_model=config.response_model,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\nasync def receive_tensor(\nshared_tensor: TensorShare,\noperation: Callable[[TensorShare], Awaitable[Any]] = Depends(\nget_computation_operation\n),\n) -&gt; Any:\n\"\"\"Asynchronous endpoint to handle tensors reception and computation.\"\"\"\nresult = await operation(shared_tensor)\nreturn result\n@router.get(\nf\"{config.ping.path}\",\nresponse_model=DefaultResponse,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\nasync def ping() -&gt; DefaultResponse:\n\"\"\"Asynchronous endpoint to check if the server is up.\"\"\"\nreturn DefaultResponse(message=\"The TensorShare router is up and running!\")\nreturn router\n</code></pre>"},{"location":"api/tensorshare_server/#src.tensorshare.server.create_tensorshare_router","title":"<code>create_tensorshare_router(server_config=None, custom_operation=None)</code>","text":"<p>Create and configure a new TensorShare router using the given configuration and computation operation.</p> <p>Parameters:</p> Name Type Description Default <code>server_config</code> <code>Optional[Dict[str, Union[str, Type[BaseModel]]]]</code> <p>The server configuration to use for the router.</p> <code>None</code> <code>custom_operation</code> <code>Optional[Callable]</code> <p>The custom computation operation to use for the <code>/receive_tensor</code> endpoint.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>APIRouter</code> <code>APIRouter</code> <p>The configured TensorShare router ready to be integrated in a FastAPI app.</p> Source code in <code>src/tensorshare/server.py</code> <pre><code>def create_tensorshare_router(\nserver_config: Optional[Dict[str, Union[str, Type[BaseModel]]]] = None,\ncustom_operation: Optional[Callable] = None,\n) -&gt; APIRouter:\n\"\"\"\n    Create and configure a new TensorShare router using the given configuration and computation operation.\n    Args:\n        server_config (Optional[Dict[str, Union[str, Type[BaseModel]]]]):\n            The server configuration to use for the router.\n        custom_operation (Optional[Callable]):\n            The custom computation operation to use for the `/receive_tensor` endpoint.\n    Returns:\n        APIRouter:\n            The configured TensorShare router ready to be integrated in a FastAPI app.\n    \"\"\"\nif custom_operation is not None and inspect.iscoroutinefunction(custom_operation):\nraise TypeError(\n\"The `custom_operation` callable must be synchronous. \"\n\"Please use the `create_async_tensorshare_router` function instead.\"\n)\nrouter = APIRouter()\n_default_config: Dict[str, Union[str, Type[BaseModel]]] = {\n\"url\": \"http://localhost:8000\",\n\"response_model\": DefaultResponse,\n}\nconfig: TensorShareServer = TensorShareServer.from_dict(\nserver_config=server_config or _default_config\n)\ntarget_operation: Callable = custom_operation or default_compute_operation\ndef get_computation_operation() -&gt; Callable:\n\"\"\"Dependency that returns the currently set computation function.\"\"\"\nreturn target_operation\n@router.post(\nf\"{config.receive_tensor.path}\",\nresponse_model=config.response_model,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\ndef receive_tensor(\nshared_tensor: TensorShare,\noperation: Callable = Depends(get_computation_operation),\n) -&gt; Any:\n\"\"\"Synchronous endpoint to handle tensors reception and computation.\"\"\"\nresult = operation(shared_tensor)\nreturn result\n@router.get(\nf\"{config.ping.path}\",\nresponse_model=DefaultResponse,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\ndef ping() -&gt; DefaultResponse:\n\"\"\"Synchronous endpoint to check if the server is up.\"\"\"\nreturn DefaultResponse(message=\"The TensorShare router is up and running!\")\nreturn router\n</code></pre>"},{"location":"api/tensorshare_server/#src.tensorshare.server.default_async_compute_operation","title":"<code>default_async_compute_operation(tensors)</code>  <code>async</code>","text":"<p>Default asynchronous action to perform when receiving a TensorShare object.</p> Source code in <code>src/tensorshare/server.py</code> <pre><code>async def default_async_compute_operation(tensors: TensorShare) -&gt; DefaultResponse:\n\"\"\"Default asynchronous action to perform when receiving a TensorShare object.\"\"\"\nreturn DefaultResponse(message=\"Success!\")\n</code></pre>"},{"location":"api/tensorshare_server/#src.tensorshare.server.default_compute_operation","title":"<code>default_compute_operation(tensors)</code>","text":"<p>Default action to perform when receiving a TensorShare object.</p> Source code in <code>src/tensorshare/server.py</code> <pre><code>def default_compute_operation(tensors: TensorShare) -&gt; DefaultResponse:\n\"\"\"Default action to perform when receiving a TensorShare object.\"\"\"\nreturn DefaultResponse(message=\"Success!\")\n</code></pre>"},{"location":"usage/backends/","title":"Backends","text":""},{"location":"usage/backends/#supported-backends","title":"Supported backends","text":"<p>The project currently supports the following ML backends:</p> <ul> <li> Flax</li> <li> NumPy</li> <li> PaddlePaddle</li> <li> TensorFlow (coming soon when <code>v2.14</code> is released)</li> <li> PyTorch</li> </ul>"},{"location":"usage/backends/#backend-enum","title":"Backend Enum","text":"<p>We provide a <code>Backend</code> Enum class to help you choose the backend you want to use or for type hinting purposes.</p> <pre><code>from tensorshare import Backend\nflax_backend = Backend.FLAX\n&gt;&gt;&gt; &lt;Backend.FLAX: \"flax\"&gt;\nnumpy_backend = Backend.NUMPY\n&gt;&gt;&gt; &lt;Backend.NUMPY: \"numpy\"&gt;\npaddlepaddle_backend = Backend.PADDLEPADDLE\n&gt;&gt;&gt; &lt;Backend.PADDLEPADDLE: \"paddlepaddle\"&gt;\ntensorflow_backend = Backend.TENSORFLOW\n&gt;&gt;&gt; &lt;Backend.TENSORFLOW: \"tensorflow\"&gt;\nbackend = Backend.TORCH\n&gt;&gt;&gt; &lt;Backend.TORCH: \"torch\"&gt;\n</code></pre> <p>Tip</p> <p>When a method requires you to specify a <code>backend</code> you can choose to use the <code>Backend</code> Enum class or the string representation of the backend. For example, the following two lines are equivalent:</p> <pre><code>ts = TensorShare(...)\ntensors = ts.to_tensors(backend=Backend.FLAX)\ntensors = ts.to_tensors(backend=\"flax\")\n</code></pre> <p>One is prone to typos. The other enables type hinting and IDE autocompletion.</p> <p>The choice is yours. \ud83d\ude09</p>"},{"location":"usage/backends/#tensortype-enum","title":"TensorType Enum","text":"<p>We also provide a <code>TensorType</code> Enum class to help you choose the tensor type you want to use or for type hinting purposes.</p> <pre><code>from tensorshare import TensorType\nflax_tensor = TensorType.FLAX\n&gt;&gt;&gt; &lt;TensorType.FLAX: \"jaxlib.xla_extension.ArrayImpl\"&gt;\nnumpy_tensor = TensorType.NUMPY\n&gt;&gt;&gt; &lt;TensorType.NUMPY: \"numpy.ndarray\"&gt;\npaddlepaddle_tensor = TensorType.PADDLEPADDLE\n&gt;&gt;&gt; &lt;TensorType.PADDLEPADDLE: \"paddle.Tensor\"&gt;\ntensorflow_tensor = TensorType.TENSORFLOW\n&gt;&gt;&gt; &lt;TensorType.TENSORFLOW: \"tensorflow.Tensor\"&gt;\ntorch_tensor = TensorType.TORCH\n&gt;&gt;&gt; &lt;TensorType.TORCH: \"torch.Tensor\"&gt;\n</code></pre>"},{"location":"usage/fastapi_integration/","title":"FastAPI Integration","text":"<p>TensorShare provides built-in integration capabilities with <code>FastAPI</code>, a high-performance web framework for building APIs with Python.</p> <p>By combining the power of TensorShare's tensor-sharing capabilities with FastAPI's intuitive and robust API development, you can easily set up an efficient server-side endpoint to handle tensor computations.</p> <p>Tip</p> <p>It's also because not using FastAPI in a project that leverages the best of open-source is a crime. Don't be a criminal.</p>"},{"location":"usage/fastapi_integration/#pre-requisites","title":"Pre-requisites","text":"<p>This module (obviously) needs <code>fastapi</code> to be installed:</p> <pre><code>pip install tensorshare[server]\n</code></pre>"},{"location":"usage/fastapi_integration/#the-pre-built-router","title":"The pre-built router","text":"<p>TensorShare provides a pre-built router that you can use to quickly integrate tensor-sharing into any existing FastAPI application.</p>"},{"location":"usage/fastapi_integration/#importing-the-router","title":"Importing the router","text":"<p>To import the router, use the following import statement:</p> SyncAsync <pre><code>from tensorshare import create_tensorshare_router\n</code></pre> <pre><code>from tensorshare import create_async_tensorshare_router\n</code></pre>"},{"location":"usage/fastapi_integration/#adding-it-to-fastapi","title":"Adding it to FastAPI","text":"<p>To add the router to your FastAPI application, add it to the <code>include</code> parameter of the <code>APIRouter</code> constructor:</p> SyncAsync <pre><code>from fastapi import FastAPI, APIRouter\nfrom tensorshare import create_tensorshare_router\napp = FastAPI()  # Your FastAPI application\nts_router = create_tensorshare_router()\napp.include_router(ts_router)\n</code></pre> <pre><code>from fastapi import FastAPI, APIRouter\nfrom tensorshare import create_async_tensorshare_router\napp = FastAPI()  # Your FastAPI application\nts_router = create_async_tensorshare_router()\napp.include_router(ts_router)\n</code></pre> <p>You can add extra <code>prefix</code> or <code>tags</code> parameter to the <code>include_router</code> method to customize the router:</p> <pre><code>app.include_router(ts_router, prefix=\"/tensorshare\", tags=[\"tensorshare\", \"tensors\"])\n</code></pre>"},{"location":"usage/fastapi_integration/#customize-the-router","title":"Customize the router","text":"<p>If you want to customize the router, you will have to pass two parameters to the <code>create_tensorshare_router</code> function:</p> <ul> <li>server_config: A Python dictionary containing the configuration of the server, just like the one you would pass to     the <code>TensorShareServer</code> constructor.</li> <li>custom_operation: The custom computation operation to use for the <code>/receive_tensor</code> endpoint. You will learn more     about this in the Customizing the computation operation section.</li> </ul> SyncAsync <pre><code>from fastapi import FastAPI, APIRouter\nfrom tensorshare import create_tensorshare_router\napp = FastAPI()  # Your FastAPI application\nserver_config = {\"url\": \"http://localhost:8000\"}\nts_router = create_tensorshare_router(server_config=server_config)\napp.include_router(ts_router)\n</code></pre> <pre><code>from fastapi import FastAPI, APIRouter\nfrom tensorshare import create_async_tensorshare_router\napp = FastAPI()  # Your FastAPI application\nserver_config = {\"url\": \"http://localhost:8000\"}\nts_router = create_async_tensorshare_router(server_config=server_config)\napp.include_router(ts_router)\n</code></pre>"},{"location":"usage/fastapi_integration/#the-ping-endpoint","title":"The <code>ping</code> endpoint","text":"<p>The router comes with a built-in <code>ping</code> endpoint that you can use to check if the server is up and running.</p> SyncAsync <pre><code>@router.get(\nf\"{config.ping.path}\",\nresponse_model=DefaultResponse,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\ndef ping() -&gt; DefaultResponse:\n\"\"\"Synchronous endpoint to check if the server is up.\"\"\"\nreturn DefaultResponse(message=\"The TensorShare router is up and running!\")\n</code></pre> <pre><code>    @router.post(\nf\"{config.receive_tensor.path}\",\nresponse_model=config.response_model,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\ndef receive_tensor(\nshared_tensor: TensorShare,\noperation: Callable = Depends(get_computation_operation),\n) -&gt; Any:\n\"\"\"Synchronous endpoint to handle tensors reception and computation.\"\"\"\nresult = operation(shared_tensor)\nreturn result\n</code></pre> <p>Nothing is too fancy here, just a simple endpoint that returns a <code>DefaultResponse</code> object with a message.</p> <p>The <code>config.ping.path</code> will be the path defined in the <code>TensorShareServer</code> configuration object you provided when creating the router.</p>"},{"location":"usage/fastapi_integration/#the-receive_tensor-endpoint","title":"The <code>receive_tensor</code> endpoint","text":"<p>The router also comes with a built-in <code>receive_tensor</code> endpoint that you can use to receive a tensor from a client.</p> SyncAsync <pre><code>@router.post(\nf\"{config.receive_tensor.path}\",\nresponse_model=config.response_model,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\ndef receive_tensor(\nshared_tensor: TensorShare,\noperation: Callable = Depends(get_computation_operation),\n) -&gt; Any:\n\"\"\"Synchronous endpoint to handle tensors reception and computation.\"\"\"\nresult = operation(shared_tensor)\nreturn result\n</code></pre> <pre><code>@router.post(\nf\"{config.receive_tensor.path}\",\nresponse_model=config.response_model,\nstatus_code=http_status.HTTP_200_OK,\ntags=[\"tensorshare\"],\n)\nasync def receive_tensor(\nshared_tensor: TensorShare,\noperation: Callable[[TensorShare], Awaitable[Any]] = Depends(\nget_computation_operation\n),\n) -&gt; Any:\n\"\"\"Asynchronous endpoint to handle tensors reception and computation.\"\"\"\nresult = await operation(shared_tensor)\nreturn result\n</code></pre> <p>Like the <code>ping</code> endpoint, the endpoint path will be defined in the <code>TensorShareServer</code> configuration object.</p>"},{"location":"usage/fastapi_integration/#customizing-the-computation-operation","title":"Customizing the computation operation","text":"<p>As you can see, there is an <code>operation</code> parameter that is a <code>Callable</code> object. This parameter is a dependency that will be resolved by the <code>get_computation_operation</code> function.</p> <p>It allows you to easily customize the computation operation that will be applied to the received tensor simply by providing a custom <code>Callable</code> object during the router creation (See Customize the router).</p> <p>Here is the behavior of the <code>get_computation_operation</code> function:</p> SyncAsync <pre><code>target_operation: Callable = custom_operation or default_compute_operation\ndef get_computation_operation() -&gt; Callable:\n\"\"\"Dependency that returns the currently set computation function.\"\"\"\nreturn target_operation\n</code></pre> <p>By default the <code>target_operation</code> variable is set to the <code>default_compute_operation</code> function, which is defined as follows:</p> <pre><code>def default_compute_operation(tensors: TensorShare) -&gt; DefaultResponse:\n\"\"\"Default action to perform when receiving a TensorShare object.\"\"\"\nreturn DefaultResponse(message=\"Success!\")\n</code></pre> <pre><code>target_operation: Callable[[TensorShare], Awaitable[Any]] = (\ncustom_operation or default_async_compute_operation\n)\ndef get_computation_operation() -&gt; Callable[[TensorShare], Awaitable[Any]]:\n\"\"\"Dependency that returns the currently set async computation function.\"\"\"\nreturn target_operation\n</code></pre> <p>By default the <code>target_operation</code> variable is set to the <code>default_async_compute_operation</code> function, which is defined as follows:</p> <pre><code>async def default_async_compute_operation(tensors: TensorShare) -&gt; DefaultResponse:\n\"\"\"Default asynchronous action to perform when receiving a TensorShare object.\"\"\"\nreturn DefaultResponse(message=\"Success!\")\n</code></pre> <p>Here is an example of a custom computation operation you could provide if you wanted to convert the received tensor to a <code>torch.Tensor</code> and return it as a list of lists of floats:</p> <pre><code>class GoBrrBrr(BaseModel):\n\"\"\"The new response_model to use for the router.\"\"\"\npredictions: List[List[float]]  # List of predictions converted to list of floats\ndef compute_go_brr_brr(tensors: TensorShare) -&gt; Dict[str, List[float]]:\n\"\"\"New computation operation to run inference on the received tensor.\"\"\"\ntorch_tensors: Dict[str, torch.Tensor] = tensors.to_tensors(backend=\"torch\")\ninference_result: List[List[float]] = []\nfor _, v in torch_tensors.items():\ny_hat = ml_pred(v)\ninference_result.append(y_hat.tolist())\nreturn {\"predictions\": inference_result}\n</code></pre> <p>Tip</p> <p>This is an elementary example, but you can do more complex things with this. You can run <code>async</code> functions or even run a whole pipeline of operations.</p> <p>Now during the router creation, you can pass the <code>compute_go_brr_brr</code> function to the <code>custom_operation</code> parameter and update the <code>response_model</code> parameter to match the <code>GoBrrBrr</code> model:</p> SyncAsync <pre><code>from fastapi import FastAPI, APIRouter\nfrom tensorshare import create_tensorshare_router\nclass GoBrrBrr(BaseModel):\n\"\"\"The new response_model to use for the router.\"\"\"\npredictions: List[List[float]]  # List of predictions converted to list of floats\ndef compute_go_brr_brr(tensors: TensorShare) -&gt; Dict[str, List[float]]:\n\"\"\"New computation operation to run inference on the received tensor.\"\"\"\ntorch_tensors: Dict[str, torch.Tensor] = tensors.to_tensors(backend=\"torch\")\ninference_result: List[List[float]] = []\nfor _, v in torch_tensors.items():\ny_hat = ml_pred(v)\ninference_result.append(y_hat.tolist())\nreturn {\"predictions\": inference_result}\napp = FastAPI()  # Your FastAPI application\nserver_config = {\"url\": \"http://localhost:8000\", \"response_model\": GoBrrBrr}\nts_router = create_tensorshare_router(\nserver_config=server_config,\ncustom_operation=compute_go_brr_brr,\n)\napp.include_router(ts_router)\n</code></pre> <pre><code>import asyncio\nfrom fastapi import FastAPI, APIRouter\nfrom tensorshare import create_async_tensorshare_router\nclass GoBrrBrr(BaseModel):\n\"\"\"The new response_model to use for the router.\"\"\"\npredictions: List[List[float]]  # List of pridctions converted to list of floats\nasync def compute_go_brr_brr(tensors: TensorShare) -&gt; Dict[str, List[float]]:\n\"\"\"New computation operation to run inference on the received tensor.\"\"\"\ntorch_tensors: Dict[str, torch.Tensor] = tensors.to_tensors(backend=\"torch\")\ninference_result: List[List[float]] = []\nfor _, v in torch_tensors.items():\ny_hat = await asyncio.run_in_executor(\nNone, ml_pred, v\n)  # Let's say ml_pred is a synchronous inference function\ninference_result.append(y_hat.tolist())\nreturn {\"predictions\": inference_result}\napp = FastAPI()  # Your FastAPI application\nserver_config = {\"url\": \"http://localhost:8000\", \"response_model\": GoBrrBrr}\nts_router = create_async_tensorshare_router(\nserver_config=server_config,\ncustom_operation=compute_go_brr_brr,\n)\napp.include_router(ts_router)\n</code></pre> <p>And voil\u00e0! \ud83c\udf89</p> <p>You now have a fully functional FastAPI application that can receive tensors from clients, run custom operations on them and return the result.</p> <p>What a breeze! \ud83d\ude0e</p>"},{"location":"usage/introduction/","title":"Introduction","text":"<p>The main goal of the TensorShare package is to provide easy to use and clear interfaces for sharing tensors between different applications connected to a network.</p> <p>The package is designed to be used in a distributed environment where multiple applications run on different machines and need to share tensors. The tensors could be in a different backend (e.g. <code>numpy</code> and <code>torch</code>) and the package will help you to share them simply and transparently.</p> <p>Tip</p> <p>The project leverages the best open-source tools and integrates them into a single package to provide you a fast and reliable way to share your tensors.</p>"},{"location":"usage/introduction/#main-components","title":"Main Components","text":"<p>There are 3 main components in the TensorShare package:</p> <ul> <li>TensorShare - the main schema for sharing tensors across the network and using multiple backends (See Backends)</li> <li>TensorShareServer - a server configuration schema defining how tensors could be shared over the network.</li> <li>TensorShareClient - the HTTP client for sending tensors to the server.</li> </ul>"},{"location":"usage/introduction/#easy-fastapi-integration","title":"Easy FastAPI Integration","text":"<p>We also provide tools to integrate TensorShare into your FastAPI application in a matter of seconds. See FastAPI Integration for more details.</p>"},{"location":"usage/introduction/#issues-and-feature-requests","title":"Issues and Feature Requests","text":"<p>If you have any issues or feature requests, please feel free to open an Issue on the GitHub repository.</p>"},{"location":"usage/tensorshare/","title":"TensorShare","text":"<p>The <code>TensorShare</code> schema is the main class of the project. It's used to share tensors between different backends.</p> <p>This schema inherits from the <code>pydantic.BaseModel</code>  class and has two fields:</p> <ul> <li><code>tensors</code>: a base64 encoded string of the serialized tensors</li> <li><code>size</code>: the size of the tensors in bytes</li> </ul>"},{"location":"usage/tensorshare/#creating-a-tensorshare-object","title":"Creating a <code>TensorShare</code> object","text":"<p>After installing the package in your project, the TensorShare class can be imported from the <code>tensorshare</code> module.</p> <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n</code></pre>"},{"location":"usage/tensorshare/#serializing-tensors-from_dict","title":"Serializing tensors - <code>from_dict</code>","text":"<p>Because it's tedious to serialize tensors manually, the package provides a <code>TensorShare.from_dict</code> method to create a new object from a dictionary of tensors in any supported backend.</p> <pre><code>from tensorshare import TensorShare\ntensors = {\n\"embeddings\": ...,  # Tensor\n\"labels\": ...,  # Tensor\n}\nts = TensorShare.from_dict(tensors)\n</code></pre>"},{"location":"usage/tensorshare/#with-a-specific-backend","title":"with a specific backend","text":"<p>You can specify the backend to use by passing the <code>backend</code> argument to the <code>from_dict</code> method.</p> <p>Tip</p> <p>The backend can be specified as a string or as a <code>Backend</code> Enum value. Check the Backends section for more information.</p> <pre><code>import torch\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": torch.zeros((2, 2)),\n\"labels\": torch.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"torch\")\nprint(ts)\n&gt;&gt;&gt; tensors=b'gAAAAAAAAAB7ImVt...' size=168\n</code></pre> <p>If you don't specify the backend, the package will try to infer it from the first tensor in the dictionary, which isn't always the best optimization. As a general rule, it's better to specify the backend explicitly.</p> <p>Warning</p> <p>It's not possible (at the moment) to mix tensors from different backends in the same dictionary. The <code>from_dict</code> method will raise an exception if you try to do so.</p>"},{"location":"usage/tensorshare/#backend-specific-examples","title":"backend-specific examples","text":"<p>Here are some examples of creating a <code>TensorShare</code> object from a dictionary of tensors in different backends.</p> FlaxNumPyPaddlePaddlePyTorchTensorFlow <pre><code>import jax.numpy as jnp\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": jnp.zeros((2, 2)),\n\"labels\": jnp.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"flax\")\n</code></pre> <pre><code>import numpy as np\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": np.zeros((2, 2)),\n\"labels\": np.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"numpy\")\n</code></pre> <pre><code>import paddle\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": paddle.zeros((2, 2)),\n\"labels\": paddle.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"paddlepaddle\")\n</code></pre> <pre><code>import torch\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": torch.zeros((2, 2)),\n\"labels\": torch.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"torch\")\n</code></pre> <pre><code>import tensorflow as tf\nfrom tensorshare import TensorShare\ntensors = {\n\"embeddings\": tf.zeros((2, 2)),\n\"labels\": tf.zeros((2, 2)),\n}\nts = TensorShare.from_dict(tensors, backend=\"tensorflow\")\n</code></pre>"},{"location":"usage/tensorshare/#deserializing-tensors","title":"Deserializing tensors","text":"<p>Like the <code>from_dict</code> method, the <code>to_tensors</code> method can be used to deserialize the serialized tensors stored in the <code>TensorShare</code> object. The method expects a <code>backend</code> argument to specify the backend to use.</p> <pre><code>ts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\ntensors = ts.to_tensors(backend=...)\n</code></pre> <p>Tip</p> <p>Again, the backend can be specified as a string or a <code>Backend</code> Enum value. Check the Backends section for more information.</p> <p>Here are some examples of deserializing the tensors from a <code>TensorShare</code> object in different backends. You must have the desired backend installed in your project to deserialize the tensors in it.</p> FlaxNumPyPaddlePaddlePyTorchTensorFlow <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n# Get a dict of jaxlib.xla_extension.ArrayImpl\ntensors_flax = ts.to_tensors(backend=\"flax\")  # or backend=Backend.FLAX\n</code></pre> <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n# Get a dict of numpy.ndarray\ntensors_numpy = ts.to_tensors(backend=\"numpy\")  # or backend=Backend.NUMPY\n</code></pre> <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n# Get a dict of paddle.Tensor\ntensors_paddle = ts.to_tensors(backend=\"paddlepaddle\")  # or backend=Backend.PADDLEPADDLE\n</code></pre> <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n# Get a dict of torch.Tensor\ntensors_pytorch = ts.to_tensors(backend=\"torch\")  # or backend=Backend.TORCH\n</code></pre> <pre><code>from tensorshare import TensorShare\nts = TensorShare(\ntensors=...,  # Base64 encoded tensors to byte strings ready to be sent\nsize=...,  # Size of the tensors in pydantic.ByteSize format\n)\n# Get a dict of tensorflow.Tensor\ntensors_tensorflow = ts.to_tensors(backend=\"tensorflow\")  # or backend=Backend.TENSORFLOW\n</code></pre>"},{"location":"usage/tensorshare/#lazy-tensors-formatting","title":"Lazy tensors formatting","text":"<p>If you don't want to handle the formatting of the tensors yourself, we provide  an utils function to prepare tensors to be used in the <code>TensorShare</code> class.</p> <pre><code>from tensorshare import prepare_tensors_to_dict\ntensors_in_any_format: Any = ...\ntensors = prepare_tensors_to_dict(tensors_in_any_format)\n&gt;&gt;&gt; {\"embeddings_0\": ..., \"embeddings_1\": ..., ...}\n</code></pre> <p>Check the utils documentation for more information.</p>"},{"location":"usage/tensorshare_client/","title":"TensorShareClient","text":"<p>The <code>TensorShareClient</code> class is a core component of this project, enabling users to send tensors to a remote server  using asynchronous HTTP requests.</p> <p>It's built on top of the <code>aiohttp</code> library, allowing asynchronous HTTP communication and enhancing the efficiency and speed of tensor sharing.</p> <p>This class extends the TensorShare ecosystem by providing both synchronous and asynchronous interfaces for  sending tensors, ensuring flexibility and adaptability to various use cases and environments.</p> <p>Note</p> <p>While it's inherently asynchronous (thanks to the `aiohttp`` library), the class provides  a synchronous interface for users who prefer it, especially for testing and non-production use.</p> <p>However, due to potential performance implications, using the synchronous method in production environments  isn't recommended.</p>"},{"location":"usage/tensorshare_client/#pre-requisites","title":"Pre-requisites","text":"<p>This module requires the <code>aiohttp</code> library to be installed:</p> <pre><code>pip install tensorshare[client]\n</code></pre>"},{"location":"usage/tensorshare_client/#initialization","title":"Initialization","text":"<p>To create an instance of <code>TensorShareClient</code>, users need to pass in the server configuration, a timeout duration (defaulted to 10 seconds), and optionally decide if they want to validate server endpoints at initialization.</p> <pre><code>from tensorshare import TensorShareServer, TensorShareClient\nserver_config = TensorShareServer.from_dict(\nserver_config={\"url\": \"http://localhost:5000\"}\n)\nclient = TensorShareClient(server_config, timeout=10, validate_endpoints=True)\n</code></pre>"},{"location":"usage/tensorshare_client/#parameters","title":"Parameters","text":"<ul> <li>server_config (<code>TensorShareServer</code>): The configuration for the remote server.</li> <li>timeout (<code>int</code>): Specifies the maximum duration (in seconds) to wait for an HTTP response. Defaults to 10 seconds.</li> <li>validate_endpoints (<code>bool</code>): Determines whether the client should validate the server's endpoints upon initialization. Defaults to True (recommended).</li> </ul>"},{"location":"usage/tensorshare_client/#methods","title":"Methods","text":"<p>The methods are available in both synchronous and asynchronous forms, with the latter being prefixed with <code>async_</code>. The asynchronous methods are recommended for production use, as they are non-blocking and more efficient.</p> <p>Note</p> <p>The synchronous methods use the asynchronous ones under the hood wrapped in an <code>asyncio.run</code> call,</p>"},{"location":"usage/tensorshare_client/#ping_server","title":"<code>ping_server</code>","text":"<p>The <code>ping_server</code> method sends a <code>GET</code> request to the server's <code>/ping</code> endpoint, which returns a <code>DefaultResponse</code> object with a <code>status_code</code> of <code>200</code> if the server is up and running.</p> SyncAsync <pre><code>from tensorshare import TensorShareServer, TensorShareClient\nserver_config = TensorShareServer.from_dict(\nserver_config={\"url\": \"http://localhost:5000\"}\n)\nclient = TensorShareClient(server_config)\nr = client.ping_server()\nprint(r)\n&gt;&gt;&gt; True\n</code></pre> <pre><code>import asyncio\nfrom tensorshare import TensorShareServer, TensorShareClient\nserver_config = TensorShareServer.from_dict(\nserver_config={\"url\": \"http://localhost:5000\"}\n)\nclient = TensorShareClient(server_config)\nasync def main():\nr = await client.async_ping_server()\nprint(r)\nasyncio.run(main())\n&gt;&gt;&gt; True\n</code></pre>"},{"location":"usage/tensorshare_client/#send_tensor","title":"<code>send_tensor</code>","text":"<p>The <code>send_tensor</code> method sends a tensor to the server's <code>/receive_tensor</code> endpoint, which returns the <code>response_model</code> defined in the server's configuration.</p> SyncAsync <pre><code>import torch\nfrom tensorshare import TensorShare, TensorShareServer, TensorShareClient\nserver_config = TensorShareServer.from_dict(\nserver_config={\"url\": \"http://localhost:5000\"}\n)\nclient = TensorShareClient(server_config)\nts = TensorShare.from_dict({\"embeddings\": torch.rand(10, 10)})\nr = client.send_tensor(ts)\n</code></pre> <pre><code>import asyncio\nimport torch\nfrom tensorshare import TensorShare, TensorShareServer, TensorShareClient\nserver_config = TensorShareServer.from_dict(\nserver_config={\"url\": \"http://localhost:5000\"}\n)\nclient = TensorShareClient(server_config)\nts = TensorShare.from_dict({\"embeddings\": torch.rand(10, 10)})\nasync def main():\nr = await client.async_send_tensor(ts)\nasyncio.run(main())\n</code></pre>"},{"location":"usage/tensorshare_server/","title":"TensorShareServer","text":"<p>The <code>TensorShareServer</code> schema defines the server configuration that will receive the tensors from any client.</p> <p>It will be used by the <code>TensorShareClient</code> for sending tensors over the network.</p>"},{"location":"usage/tensorshare_server/#schema","title":"Schema","text":"<p>The schema inherits from the <code>pydantic.BaseModel</code> class and has the following properties:</p> <ul> <li> <p><code>url</code>: The URL of the server. This is the URL that the client will use to connect to the server. It should be a valid URL, including the protocol (e.g. <code>http://localhost:5000</code> or <code>https://example.com</code>).</p> </li> <li> <p><code>ping</code>: The path to the health-check endpoint. This is the path that the client will use to check if the server is available. It should be a valid string, without the leading slash (e.g. <code>ping</code> or <code>healthcheck</code>). It will be <code>ping</code> by default.</p> </li> <li> <p><code>receive_tensor</code>: The path to the endpoint receiving the tensor. This is the path that the client will use to send the tensor to the server. It should be a valid string without the leading slash (e.g. <code>receive_tensor</code> or <code>tensors</code>). It will be <code>receive_tensor</code> by default.</p> </li> <li> <p><code>response_model</code>: The response model that the server will return to the client. It should be a subclass of <code>BaseModel</code> from Pydantic. It will be <code>DefaultResponse</code> by default, a simple model returning a <code>message</code> string.</p> </li> </ul>"},{"location":"usage/tensorshare_server/#create-a-config-from_dict","title":"Create a config - <code>from_dict</code>","text":"<p>The <code>from_dict</code> method creates a <code>TensorShareServer</code> config from a dictionary.</p> <pre><code>from tensorshare import DefaultResponse, TensorShareServer\nconfig = TensorShareServer.from_dict(\nserver_config={\n\"url\": \"http://localhost:5000\",      # required\n\"ping\": \"ping\",                      # optional\n\"receive_tensor\": \"receive_tensor\",  # optional\n\"response_model\": DefaultResponse,   # optional\n}\n)\nprint(config)\n# url=Url('http://localhost:5000/')\n# ping=Url('http://localhost:5000/ping')\n# receive_tensor=Url('http://localhost:5000/receive_tensor')\n# response_model=&lt;class 'tensorshare.schema.DefaultResponse'&gt;\n</code></pre> <p>The <code>url</code>, <code>ping</code> and <code>receive_tensor</code> properties leverage the <code>HttpUrl</code> type from Pydantic. This means that they will be validated and normalized at runtime.</p>"},{"location":"usage/tensorshare_server/#use-the-config","title":"Use the config","text":"<p>Like any <code>pydantic</code> model, you can use the config as a standard Python class object.</p> <pre><code>config.url\n&gt;&gt;&gt; Url('http://localhost:5000/')\nconfig.ping\n&gt;&gt;&gt; Url('http://localhost:5000/ping')\nconfig.receive_tensor\n&gt;&gt;&gt; Url('http://localhost:5000/receive_tensor')\nconfig.response_model\n&gt;&gt;&gt; &lt;class 'tensorshare.schema.DefaultResponse'&gt;\n</code></pre> <p>You can also get the URL as a string or some parts of it.</p> <pre><code>str(config.url)\n&gt;&gt;&gt; 'http://localhost:5000/'\nconfig.url.scheme\n&gt;&gt;&gt; 'http'\nconfig.url.host\n&gt;&gt;&gt; 'localhost'\nconfig.url.port\n&gt;&gt;&gt; 5000\nconfig.url.path\n&gt;&gt;&gt; '/'\n</code></pre>"}]}